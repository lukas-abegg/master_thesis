{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read WIKILARGE article sentences dataset:\n",
      "loaded..........\n"
     ]
    }
   ],
   "source": [
    "wikilarge = \"../../data/wikilarge\"\n",
    "\n",
    "print(\"Read WIKILARGE article sentences dataset:\")\n",
    "\n",
    "fp_train_src = 'train.src'\n",
    "fp_train_src = os.path.join(wikilarge, fp_train_src)\n",
    "df_train_src = pd.read_csv(fp_train_src, sep=\"\\n\", header=None)\n",
    "df_train_src.columns = [\"sentence\"]\n",
    "\n",
    "fp_train_dst = 'train.dst'\n",
    "fp_train_dst = os.path.join(wikilarge, fp_train_dst)\n",
    "df_train_dst = pd.read_csv(fp_train_dst, sep=\"\\n\", header=None)\n",
    "df_train_dst.columns = [\"sentence\"]\n",
    "\n",
    "fp_valid_src = 'valid.src'\n",
    "fp_valid_src = os.path.join(wikilarge, fp_valid_src)\n",
    "df_valid_src = pd.read_csv(fp_valid_src, sep=\"\\n\", header=None)\n",
    "df_valid_src.columns = [\"sentence\"]\n",
    "\n",
    "fp_valid_dst = 'valid.dst'\n",
    "fp_valid_dst = os.path.join(wikilarge, fp_valid_dst)\n",
    "df_valid_dst = pd.read_csv(fp_valid_dst, sep=\"\\n\", header=None)\n",
    "df_valid_dst.columns = [\"sentence\"]\n",
    "\n",
    "fp_test_src = 'test.src'\n",
    "fp_test_src = os.path.join(wikilarge, fp_test_src)\n",
    "df_test_src = pd.read_csv(fp_test_src, sep=\"\\n\", header=None)\n",
    "df_test_src.columns = [\"sentence\"]\n",
    "\n",
    "fp_test_dst = 'test.dst'\n",
    "fp_test_dst = os.path.join(wikilarge, fp_test_dst)\n",
    "df_test_dst = pd.read_csv(fp_test_dst, sep=\"\\n\", header=None)\n",
    "df_test_dst.columns = [\"sentence\"]\n",
    "\n",
    "print(\"loaded..........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of sentences:\n",
      "....................\n",
      "df_train_src: 296402\n",
      "df_train_dst: 296402\n",
      "df_valid_src: 992\n",
      "df_valid_dst: 992\n",
      "df_test_src: 359\n",
      "df_test_dst: 359\n"
     ]
    }
   ],
   "source": [
    "print(\"Amount of sentences:\")\n",
    "print(\"....................\")\n",
    "print(\"df_train_src:\", len(df_train_src))\n",
    "print(\"df_train_dst:\", len(df_train_dst))\n",
    "print(\"df_valid_src:\", len(df_valid_src))\n",
    "print(\"df_valid_dst:\", len(df_valid_dst))\n",
    "print(\"df_test_src:\", len(df_test_src))\n",
    "print(\"df_test_dst:\", len(df_test_dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize in WordPiece\n",
      "tokenized....................\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenize in WordPiece\")\n",
    "\n",
    "x1 = df_train_src[\"sentence\"].map(lambda x: bert_tokenizer.tokenize(x))\n",
    "x2 = df_train_dst[\"sentence\"].map(lambda x: bert_tokenizer.tokenize(x))\n",
    "x3 = df_valid_src[\"sentence\"].map(lambda x: bert_tokenizer.tokenize(x))\n",
    "x4 = df_valid_dst[\"sentence\"].map(lambda x: bert_tokenizer.tokenize(x))\n",
    "x5 = df_test_src[\"sentence\"].map(lambda x: bert_tokenizer.tokenize(x))\n",
    "x6 = df_test_dst[\"sentence\"].map(lambda x: bert_tokenizer.tokenize(x))\n",
    "\n",
    "x1_len = x1.map(lambda x: len(x))\n",
    "x2_len = x2.map(lambda x: len(x))\n",
    "x3_len = x3.map(lambda x: len(x))\n",
    "x4_len = x4.map(lambda x: len(x))\n",
    "x5_len = x5.map(lambda x: len(x))\n",
    "x6_len = x6.map(lambda x: len(x))\n",
    "\n",
    "print(\"tokenized....................\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize in Words\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenize in Words\")\n",
    "\n",
    "w1 = df_train_src[\"sentence\"].map(lambda w: tokenize_en(w))\n",
    "w2 = df_train_dst[\"sentence\"].map(lambda w: tokenize_en(w))\n",
    "w3 = df_valid_src[\"sentence\"].map(lambda w: tokenize_en(w))\n",
    "w4 = df_valid_dst[\"sentence\"].map(lambda w: tokenize_en(w))\n",
    "w5 = df_test_src[\"sentence\"].map(lambda w: tokenize_en(w))\n",
    "w6 = df_test_dst[\"sentence\"].map(lambda w: tokenize_en(w))\n",
    "\n",
    "w1_len = w1.map(lambda w: len(w))\n",
    "w2_len = w2.map(lambda w: len(w))\n",
    "w3_len = w3.map(lambda w: len(w))\n",
    "w4_len = w4.map(lambda w: len(w))\n",
    "w5_len = w5.map(lambda w: len(w))\n",
    "w6_len = w6.map(lambda w: len(w))\n",
    "\n",
    "print(\"tokenized....................\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analyse\")\n",
    "\n",
    "df_src_x = pd.concat([x1_len, x3_len, x5_len])\n",
    "df_dst_x = pd.concat([x2_len, x4_len, x6_len])\n",
    "\n",
    "df_src_w = pd.concat([w1_len, w3_len, w5_len])\n",
    "df_dst_w = pd.concat([w2_len, w4_len, w6_len])\n",
    "\n",
    "df_src_x_99 = df_src_x[df_src_x < df_src_x.quantile(.99)]\n",
    "df_dst_x_99 = df_dst_x[df_dst_x < df_dst_x.quantile(.99)]\n",
    "\n",
    "df_src_w_99 = df_src_w[df_src_w < df_src_w.quantile(.99)]\n",
    "df_dst_w_99 = df_dst_w[df_dst_w < df_dst_w.quantile(.99)]\n",
    "\n",
    "max_df_src_x = df_src_x.max()\n",
    "quantile_75_df_src_x = df_src_x.quantile(.75)\n",
    "quantile_99_df_src_x = df_src_x.quantile(.99)\n",
    "mean_df_src_x = df_src_x_99.mean()\n",
    "\n",
    "max_df_src_w = df_src_w_99.max()\n",
    "quantile_75_df_src_w = df_src_w_99.quantile(.75)\n",
    "quantile_99_df_src_w = df_src_w_99.quantile(.99)\n",
    "mean_df_src_w = df_src_w_99.mean()\n",
    "\n",
    "max_df_dst_x = df_dst_x.max()\n",
    "quantile_75_df_dst_x = df_dst_x.quantile(.75)\n",
    "quantile_99_df_dst_x = df_dst_x.quantile(.99)\n",
    "mean_df_dst_x = df_dst_x_99.mean()\n",
    "\n",
    "max_df_dst_w = df_src_w_99.max()\n",
    "quantile_75_df_dst_w = df_dst_w_99.quantile(.75)\n",
    "quantile_99_df_dst_w = df_dst_w_99.quantile(.99)\n",
    "mean_df_dst_w = df_dst_w_99.mean()\n",
    "\n",
    "print(\"analysed....................\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SRC Wordpiece....................\")\n",
    "print(\"max_df_src_x:\", max_df_src_x)\n",
    "print(\"quantile_75_df_src_x:\", quantile_75_df_src_x)\n",
    "print(\"quantile_99_df_src_x:\", quantile_99_df_src_x)\n",
    "print(\"mean_df_src_x:\", mean_df_src_x)\n",
    "\n",
    "print(\"SRC Word....................\")\n",
    "print(\"max_df_src_w:\", max_df_src_w)\n",
    "print(\"quantile_75_df_src_w:\", quantile_75_df_src_w)\n",
    "print(\"quantile_99_df_src_w:\", quantile_99_df_src_w)\n",
    "print(\"mean_df_src_w:\", mean_df_src_w)\n",
    "print(\"\\n\")\n",
    "print(\"DST Wordpiece....................\")\n",
    "print(\"max_df_dst_x:\", max_df_dst_x)\n",
    "print(\"quantile_75_df_dst_x:\", quantile_75_df_dst_x)\n",
    "print(\"quantile_99_df_dst_x:\", quantile_99_df_dst_x)\n",
    "print(\"mean_df_dst_x:\", mean_df_dst_x)\n",
    "\n",
    "print(\"DST Word....................\")\n",
    "print(\"max_df_dst_w:\", max_df_dst_w)\n",
    "print(\"quantile_75_df_dst_w:\", quantile_75_df_dst_w)\n",
    "print(\"quantile_99_df_dst_w:\", quantile_99_df_dst_w)\n",
    "print(\"mean_df_dst_w:\", mean_df_dst_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({'figure.figsize':(5,3), 'figure.dpi':200})\n",
    "kwargs = dict(alpha=0.5, bins=100, density=True, stacked=True)\n",
    "\n",
    "# Plot Histogram on x\n",
    "plt.axvline(x=quantile_75_df_src_x,color='lightgreen', label='75th quantile source sequences')\n",
    "plt.axvline(x=quantile_75_df_dst_x,color='violet', label='75th quantile target sequences')\n",
    "plt.hist(df_src_x_99, **kwargs, color='g', label='source sentences')\n",
    "plt.hist(df_dst_x_99, **kwargs, color='b', label='target sequences')\n",
    "plt.gca().set(title='Word Pieces Sequence Length Histogram: WIKILARGE Dataset 99th Quantile', ylabel='Frequency', xlabel=\"Length\");\n",
    "plt.legend(loc='lower left', bbox_to_anchor= (1.1, 0.6), ncol=1,\n",
    "            borderaxespad=0, frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge tokenized sentences Wordpiece\n",
    "df_src_x_tok = pd.concat([x1, x3, x5])\n",
    "df_dst_x_tok = pd.concat([x2, x4, x6])\n",
    "\n",
    "frame = { 'src': df_src_x_tok, 'dst': df_dst_x_tok } \n",
    "df_src_dst_tok = pd.DataFrame(frame) \n",
    "\n",
    "print(\"Amount of src/dst\", len(df_src_dst_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge tokenized sentences Words\n",
    "df_src_w_tok = pd.concat([w1, w3, w5])\n",
    "df_dst_w_tok = pd.concat([w2, w4, w6])\n",
    "\n",
    "frame = { 'src': df_src_w_tok, 'dst': df_dst_w_tok } \n",
    "df_src_dst_w_tok = pd.DataFrame(frame) \n",
    "\n",
    "print(\"Amount of src/dst\", len(df_src_dst_w_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove sentence pairs with over .99 percentile length from WordPiece\n",
    "print(\"quantile_99_df_src_x\", quantile_99_df_src_x)\n",
    "print(\"quantile_99_df_dst_x\", quantile_99_df_dst_x)\n",
    "\n",
    "index_drop = []\n",
    "for index, row in df_src_dst_tok.iterrows():\n",
    "    if len(row.src) > quantile_99_df_src_x and len(row.dst) > quantile_99_df_dst_x:\n",
    "        index_drop.append(index)\n",
    "print(index_drop)\n",
    "print(len(df_src_dst_tok))\n",
    "df_src_dst_tok = df_src_dst_tok.drop(df_src_dst_tok.index[index_drop])\n",
    "print(len(df_src_dst_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(df_src_dst_w_tok))\n",
    "df_src_dst_w_tok = df_src_dst_w_tok.drop(df_src_dst_w_tok.index[index_drop])\n",
    "print(len(df_src_dst_w_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove sentence pairs with over .99 percentile length from WordPiece for single sets\n",
    "print(\"quantile_99_df_src_x\", quantile_99_df_src_x)\n",
    "print(\"quantile_99_df_dst_x\", quantile_99_df_dst_x)\n",
    "\n",
    "# merge tokenized sentences Wordpiece\n",
    "frame_train = { 'src': x1, 'dst': x2 } \n",
    "df_src_dst_train = pd.DataFrame(frame_train) \n",
    "\n",
    "frame_valid = { 'src': x3, 'dst': x4 } \n",
    "df_src_dst_valid = pd.DataFrame(frame_valid) \n",
    "\n",
    "frame_test = { 'src': x5, 'dst': x6 } \n",
    "df_src_dst_test = pd.DataFrame(frame_test)\n",
    "\n",
    "index_drop = []\n",
    "for index, row in df_src_dst_train.iterrows():\n",
    "    if len(row.src) > quantile_99_df_src_x and len(row.dst) > quantile_99_df_dst_x:\n",
    "        index_drop.append(index)\n",
    "print(index_drop)\n",
    "print(len(df_src_dst_train))\n",
    "df_src_dst_train = df_src_dst_train.drop(df_src_dst_train.index[index_drop])\n",
    "print(len(df_src_dst_train))\n",
    "\n",
    "index_drop = []\n",
    "for index, row in df_src_dst_valid.iterrows():\n",
    "    if len(row.src) > quantile_99_df_src_x and len(row.dst) > quantile_99_df_dst_x:\n",
    "        index_drop.append(index)\n",
    "print(index_drop)\n",
    "print(len(df_src_dst_valid))\n",
    "df_src_dst_valid = df_src_dst_valid.drop(df_src_dst_valid.index[index_drop])\n",
    "print(len(df_src_dst_valid))\n",
    "\n",
    "index_drop = []\n",
    "for index, row in df_src_dst_test.iterrows():\n",
    "    if len(row.src) > quantile_99_df_src_x and len(row.dst) > quantile_99_df_dst_x:\n",
    "        index_drop.append(index)\n",
    "print(index_drop)\n",
    "print(len(df_src_dst_test))\n",
    "df_src_dst_test = df_src_dst_test.drop(df_src_dst_test.index[index_drop])\n",
    "print(len(df_src_dst_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Build Vocab of WordPiece\")\n",
    "\n",
    "# Create an empty dictionary \n",
    "src = dict()\n",
    "dst = dict()\n",
    "\n",
    "def sentence_count(sent, d):\n",
    "    for word in sent: \n",
    "        if word in d:\n",
    "            d[word] = d[word] + 1\n",
    "        else: \n",
    "            d[word] = 1\n",
    "\n",
    "df_src_x_99 = df_src_dst_tok.src\n",
    "df_dst_x_99 = df_src_dst_tok.dst\n",
    "\n",
    "print(len(df_src_x_99))\n",
    "print(len(df_dst_x_99))\n",
    "\n",
    "df_src_x_99.map(lambda x: sentence_count(x, src))\n",
    "df_dst_x_99.map(lambda x: sentence_count(x, dst))\n",
    "  \n",
    "# Merge Dicts\n",
    "merged = {**src, **dst}\n",
    "\n",
    "print(\"built....................\")\n",
    "\n",
    "print(\"Size of SRC-Vocab:\", len(src))\n",
    "print(\"Size of DST-Vocab:\", len(dst))\n",
    "print(\"Size of merged Vocab:\", len(merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Build Vocab of Words\")\n",
    "\n",
    "# Create an empty dictionary \n",
    "src_w = dict()\n",
    "dst_w = dict()\n",
    "\n",
    "def sentence_count(sent, d):\n",
    "    for word in sent: \n",
    "        if word in d:\n",
    "            d[word] = d[word] + 1\n",
    "        else: \n",
    "            d[word] = 1\n",
    "\n",
    "df_src_w_99 = df_src_dst_w_tok.src\n",
    "df_dst_w_99 = df_src_dst_w_tok.dst\n",
    "\n",
    "print(len(df_src_w_99))\n",
    "print(len(df_dst_w_99))\n",
    "\n",
    "df_src_w_99.map(lambda w: sentence_count(w, src_w))\n",
    "df_dst_w_99.map(lambda w: sentence_count(w, dst_w))\n",
    "  \n",
    "# Merge Dicts\n",
    "merged_w = {**src_w, **dst_w}\n",
    "\n",
    "print(\"built....................\")\n",
    "\n",
    "print(\"Size of SRC-Vocab:\", len(src_w))\n",
    "print(\"Size of DST-Vocab:\", len(dst_w))\n",
    "print(\"Size of merged Vocab:\", len(merged_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analyse Vocab\")\n",
    "\n",
    "v_merged_data = list(merged.values())\n",
    "v_df_merged_x = pd.DataFrame(v_merged_data) \n",
    "\n",
    "v_merged_w_data = list(merged_w.values())\n",
    "v_df_merged_w = pd.DataFrame(v_merged_w_data) \n",
    "\n",
    "\n",
    "max_df_merged_x = v_df_merged_x.max()\n",
    "quantile_75_df_merged_x = v_df_merged_x.quantile(.75)\n",
    "mean_df_merged_x = v_df_merged_x.mean()\n",
    "\n",
    "max_df_merged_w = v_df_merged_w.max()\n",
    "quantile_75_df_merged_w = v_df_merged_w.quantile(.75)\n",
    "mean_df_merged_w = v_df_merged_w.mean()\n",
    "\n",
    "print(\"analysed....................\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analyse Vocab\")\n",
    "\n",
    "v_merged_data = list(merged.values())\n",
    "v_df_merged_x = pd.DataFrame(v_merged_data) \n",
    "\n",
    "v_merged_w_data = list(merged_w.values())\n",
    "v_df_merged_w = pd.DataFrame(v_merged_w_data) \n",
    "\n",
    "\n",
    "max_df_merged_x = v_df_merged_x.max()\n",
    "quantile_75_df_merged_x = v_df_merged_x.quantile(.75)\n",
    "mean_df_merged_x = v_df_merged_x.mean()\n",
    "\n",
    "max_df_merged_w = v_df_merged_w.max()\n",
    "quantile_75_df_merged_w = v_df_merged_w.quantile(.75)\n",
    "mean_df_merged_w = v_df_merged_w.mean()\n",
    "\n",
    "print(\"analysed....................\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WordPiece Vocab..............\")\n",
    "print(\"max_df_merged_x\", max_df_merged_x)\n",
    "print(\"quantile_75_df_merged_x\", quantile_75_df_merged_x)\n",
    "print(\"mean_df_merged_x\", mean_df_merged_x)\n",
    "\n",
    "print(\"Word Vocab..............\")\n",
    "print(\"max_df_merged_w\", max_df_merged_w)\n",
    "print(\"quantile_75_df_merged_w\", quantile_75_df_merged_w)\n",
    "print(\"mean_df_merged_w\", mean_df_merged_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_df_merged_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_df_merged_w.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}